from __future__ import annotations

# Metric templates are categorized by what data they need:
#
# TRACE-COMPATIBLE (works without human labels):
#   - Uses: input, output, trace, metadata
#   - Examples: latency_ms, json_valid, tool_call_count, LLM judge metrics
#
# REFERENCE-BASED (needs human_label.reference):
#   - Uses: output vs human-provided reference text
#   - Examples: rouge_l, bleu, token_overlap_f1
#   - Will return N/A if no reference is provided
#
# SCOPE defines what part of the trace the metric applies to:
#   - "overall": Evaluates the final function output (default)
#   - "llm_call": Evaluates individual LLM API call outputs
#   - "tool_call": Evaluates individual tool call results
#   - "trace": Aggregates over the entire trace (counts, ratios)

OBJECTIVE_TEMPLATES = [
    # === TRACE-COMPATIBLE METRICS (no human labels needed) ===
    {
        "id": "latency_ms",
        "type": "objective",
        "description": "Measure execution latency in milliseconds.",
        "config": {},
        "category": "efficiency",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "cost",
        "type": "objective",
        "description": "Total LLM cost from trace events.",
        "config": {},
        "category": "efficiency",
        "scope": "trace",
        "requires_reference": False,
    },
    {
        "id": "json_valid",
        "type": "objective",
        "description": "Checks whether output parses as JSON.",
        "config": {},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "regex_match",
        "type": "objective",
        "description": "Checks output against a regex pattern.",
        "config": {"pattern": ""},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "token_length",
        "type": "objective",
        "description": "Checks output length (chars) against a maximum.",
        "config": {"max_chars": None},
        "category": "efficiency",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "tool_call_count",
        "type": "objective",
        "description": "Counts tool-related events in the trace.",
        "config": {},
        "category": "robustness",
        "scope": "trace",
        "requires_reference": False,
    },
    {
        "id": "output_nonempty",
        "type": "objective",
        "description": "PASS if output is not empty/None.",
        "config": {},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "output_length_range",
        "type": "objective",
        "description": "PASS if output length is within specified range.",
        "config": {"min_chars": 0, "max_chars": None},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "llm_call_count",
        "type": "objective",
        "description": "Count LLM API calls in trace.",
        "config": {"request_kind": ".request"},
        "category": "efficiency",
        "scope": "trace",
        "requires_reference": False,
    },
    {
        "id": "llm_error_rate",
        "type": "objective",
        "description": "Error rate of LLM calls based on trace events.",
        "config": {"request_kind": ".request", "error_kind": ".error"},
        "category": "robustness",
        "scope": "trace",
        "requires_reference": False,
    },
    {
        "id": "tool_success_ratio",
        "type": "objective",
        "description": "Ratio of successful tool calls to total tool calls.",
        "config": {"success_kind": "tool.success", "error_kind": "tool.error"},
        "category": "robustness",
        "scope": "trace",
        "requires_reference": False,
    },
    {
        "id": "tool_error_count",
        "type": "objective",
        "description": "Count of tool errors in trace.",
        "config": {"error_kind": "tool.error"},
        "category": "robustness",
        "scope": "trace",
        "requires_reference": False,
    },
    {
        "id": "csv_valid",
        "type": "objective",
        "description": "Checks whether output parses as CSV.",
        "config": {"dialect": "excel"},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "xml_valid",
        "type": "objective",
        "description": "Checks whether output parses as XML.",
        "config": {},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "url_count",
        "type": "objective",
        "description": "Counts URLs in the output (proxy for citations).",
        "config": {"pattern": "https?://", "min_count": 1},
        "category": "grounding",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "json_schema_keys",
        "type": "objective",
        "description": "Check JSON output includes required keys.",
        "config": {"required_keys": []},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "json_types_match",
        "type": "objective",
        "description": "Check JSON key types match expected schema.",
        "config": {"schema": {}},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "json_path_present",
        "type": "objective",
        "description": "Check required JSON paths exist (dot notation).",
        "config": {"paths": []},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "regex_capture_count",
        "type": "objective",
        "description": "Count regex matches and enforce a minimum count.",
        "config": {"pattern": "", "min_count": 1},
        "category": "structure",
        "scope": "overall",
        "requires_reference": False,
    },
    {
        "id": "pass_at_k",
        "type": "objective",
        "description": "Probability at least one of top-k candidates succeeds.",
        "config": {"k": 5, "candidate_field": "candidates", "success_field": "passed"},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": False,
    },
    # === REFERENCE-BASED METRICS (need human_label.reference) ===
    {
        "id": "bleu",
        "type": "objective",
        "description": "Text similarity using BLEU (needs human_label.reference).",
        "config": {},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "rouge_l",
        "type": "objective",
        "description": "ROUGE-L similarity (needs human_label.reference).",
        "config": {},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "rouge_1",
        "type": "objective",
        "description": "ROUGE-1 unigram overlap (needs human_label.reference).",
        "config": {},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "rouge_2",
        "type": "objective",
        "description": "ROUGE-2 bigram overlap (needs human_label.reference).",
        "config": {},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "token_overlap_f1",
        "type": "objective",
        "description": "Token overlap F1 (needs human_label.reference).",
        "config": {},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "jaccard_similarity",
        "type": "objective",
        "description": "Jaccard similarity (needs human_label.reference).",
        "config": {},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "numeric_mae",
        "type": "objective",
        "description": "Mean absolute error (needs human_label.reference).",
        "config": {"output_field": None},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "numeric_rmse",
        "type": "objective",
        "description": "Root mean squared error (needs human_label.reference).",
        "config": {"output_field": None},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "numeric_rel_error",
        "type": "objective",
        "description": "Relative error (needs human_label.reference).",
        "config": {"output_field": None},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
    {
        "id": "numeric_within_tolerance",
        "type": "objective",
        "description": "Pass if error within tolerance (needs human_label.reference).",
        "config": {"output_field": None, "tolerance": 0.0},
        "category": "correctness",
        "scope": "overall",
        "requires_reference": True,
    },
]

# Subjective metrics use LLM judges to evaluate output quality
# All subjective metrics are TRACE-COMPATIBLE (no reference needed)
# Import from canonical source in subjective.py
from .subjective import SUBJECTIVE_TEMPLATES
